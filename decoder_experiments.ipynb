{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.layers import Lambda, Input, Embedding, Dense, TimeDistributed, Dropout, BatchNormalization, Add, SpatialDropout1D\n",
    "from tensorflow.keras.layers import CuDNNLSTM as LSTM\n",
    "\n",
    "vocab_size=30000\n",
    "inp=Input(batch_shape=(1,1,),dtype=tf.int64,name=\"input_sent\")\n",
    "emb_layer=Embedding(input_dim=vocab_size,output_dim=1500,embeddings_initializer=tf.keras.initializers.Constant(0.01))\n",
    "emb=emb_layer(inp)\n",
    "emb_do=SpatialDropout1D(0.5)(emb, training=False)\n",
    "emb_drop_n=BatchNormalization()(emb_do)\n",
    "rnn1=LSTM(1500,return_sequences=True,stateful=True)(emb_drop_n)\n",
    "rnn1_n=BatchNormalization()(rnn1)\n",
    "rnn2=LSTM(1500,return_sequences=True,stateful=True)(rnn1_n)\n",
    "rnn2_n=BatchNormalization()(rnn2)\n",
    "rnn3=LSTM(1500,return_sequences=True,stateful=True)(rnn2_n)\n",
    "rnn3_n=BatchNormalization()(rnn3)\n",
    "proj_weights=TimeDistributed(Dense(1500))(Add()([rnn1_n,rnn2_n,rnn3_n]))\n",
    "proj_weights_gelu=Lambda(lambda x: tf.multiply(x,tf.nn.sigmoid(tf.scalar_mul(1.702,x))))(proj_weights)\n",
    "dec=Dense(vocab_size,activation=\"softmax\",name=\"decision\")\n",
    "dec_td=TimeDistributed(dec)(proj_weights_gelu)\n",
    "mod=tf.keras.Model(inputs=[inp],outputs=[dec_td])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "mod.load_weights(\"models/epoch.2019-03-24-00-00.00020.last.rnnlm\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "dp=data.SubwordDataPipeline(\"models/spiece_vocab.sp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.subword_model.IdToPiece(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(m):\n",
    "    lstms=[l for l in m.layers if isinstance(l,tf.keras.layers.CuDNNLSTM)]\n",
    "    for l in lstms:\n",
    "        l.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prewarm: ▁yliopiston\n",
      "Prewarm: ▁on\n",
      "Prewarm: eile\n",
      "Prewarm: ▁myös\n",
      "Prewarm: ▁kaksi\n",
      "Prewarm: ▁sata\n",
      "Prewarm: ▁ihmistä\n",
      "Prewarm: tista\n",
      "Prewarm: ▁perin\n",
      "Prewarm: uto\n",
      "Prewarm: ous\n",
      "Prewarm: ▁.\n",
      "Prewarm: M\n",
      "Prewarm: vää\n",
      "Prewarm: ▁ole\n",
      "Prewarm: ▁ole\n",
      "Prewarm: ▁riitä\n",
      "Prewarm: itä\n",
      "Prewarm: ▁,\n",
      "Prewarm: ▁pituutta\n",
      "Prewarm: ▁.\n",
      "Prewarm: ▁vaan\n",
      "Prewarm: ▁siksi\n",
      "Prewarm: ▁onkin\n",
      "Prewarm: kin\n",
      "Prewarm: ▁ei\n",
      "Prewarm: ▁ole\n",
      "Prewarm: tä\n",
      "Prewarm: ▁tarjoamaan\n",
      "Prewarm: ▁mitään\n",
      "Prewarm: al\n",
      "Prewarm: oita\n",
      "Prewarm: ▁.\n",
      "Prewarm: ▁sen\n",
      "Prewarm: ▁perusteella\n",
      "Prewarm: ▁,\n",
      "Final output:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mitä reittiä on tarkoitus kulkea . Liikenne on myös erittäin suosittu . Vesihuoltolaitoksen toiminta-alueen ulkopuolella on myös useita muita kaupunkeja , kuten esimerkiksi Hakaniemen ja Vesihuoltolaitoksen . Vesihuoltolaitos huolehtii vesihuoltolaitoksen vesihuoltolaitoksen vesihuoltolaitoksen toiminnasta . Vesihuoltolaitos huolehtii vesihuoltolaitoksen vesihuoltolaitoksen toiminta-alueesta . Vesihuoltolaitoksen toiminta-alue on vesihuoltolaitos . Vesihuoltolaitos huolehtii vesihuoltolaitoksen toiminnasta vesihuoltolaitoksen vesihuoltolaitoksen vesihuoltolaitoksen'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#winner=tf.argmax(res,axis=-1)\n",
    "#print(winner[0][0].numpy())\n",
    "#dp.subword_model.IdToPiece(int(winner))\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def prewarm(txt,mod,dp):\n",
    "    reset_model(mod)\n",
    "    ids=dp.subword_model.EncodeAsIds(txt)\n",
    "    for i in ids:\n",
    "        inp=np.asarray([[i]],dtype=np.int64)\n",
    "        inp_t=tf.convert_to_tensor(inp,tf.int64)\n",
    "        res=mod(inp_t)\n",
    "        winner=tf.argmax(res,axis=-1)\n",
    "        spiece_id=winner[0][0].numpy()\n",
    "        sw=dp.subword_model.IdToPiece(int(spiece_id))\n",
    "        print(\"Prewarm:\",sw)\n",
    "    winner=tf.argmax(res,axis=-1)\n",
    "    spiece_id=winner[0][0].numpy()\n",
    "    return int(spiece_id)\n",
    "\n",
    "#prewarm_text=\"Meillä on siis pieni ongelma. Tai no oikeastaan aika suuri sellainen. 13-vuotias poikani haluaa nimittäin viikonloppuna mennä kaverinsa ja tämän perheen kanssa Ruotsin risteilylle. Kaveri siis itse ehdotti tätä ja poika haluaa todellakin mennä. Kaikki olisi minullekin täysin OK, jos poikani ei sattuisi olemaan\"\n",
    "prewarm_text=\"Helsingin alla risteilee jopa yli satavuotisia vesiputkia. Ikä ei kuitenkaan yksin määritä putken kuntoa, ja siksi Helsingissäkään ei pystytä tekemään riskiarvioita vain sen perusteella\"\n",
    "#prewarm_text=\"Kaupunkien tarpeet ovat Suomessa jääneet lähes tyystin sivuun. Viime vuosina on kuitenkin nähty, että Helsingissä ja sen naapureissa Espoossa ja Vantaalla vuokrat nousevat ja asumisen väljyys\"\n",
    "#prewarm_text=\"Vantaan terveysasemille hankittiin uusi puhelinpalvelu: soittajat jonottavat jopa 45 minuuttia\"\n",
    "#prewarm_text=\"Olipa kerran pieni prinsessa\"\n",
    "start=prewarm(prewarm_text,mod,dp)\n",
    "lst=[]\n",
    "for _ in range(100):\n",
    "    inp=np.asarray([[start]],dtype=np.int64)\n",
    "    inp_t=tf.convert_to_tensor(inp,tf.int64)\n",
    "    res=mod(inp_t)\n",
    "    probs=tf.squeeze(res).numpy()\n",
    "    winner=sample(probs,0.5)\n",
    "    #winner=np.random.choice(a=np.arange(probs.shape[0]),p=probs)\n",
    "    #print(\"argmax:\",tf.argmax(res,axis=-1))\n",
    "    #print(\"winner:\",winner)\n",
    "    spiece_id=winner\n",
    "    sw=dp.subword_model.IdToPiece(int(spiece_id))\n",
    "    lst.append(sw)\n",
    "    start=spiece_id\n",
    "print(\"Final output:\")\n",
    "dp.subword_model.DecodePieces(lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def perplexity(text, model, data_pipeline):\n",
    "    \n",
    "    from tensorflow.keras.metrics import sparse_categorical_crossentropy\n",
    "    from tensorflow.keras.backend import exp\n",
    "    \n",
    "    reset_model(model)\n",
    "    text_ids=data_pipeline.subword_model.EncodeAsIds(text)\n",
    "    predictions=[]\n",
    "    for subword_id in text_ids[:-1]:\n",
    "        inp=np.asarray([[subword_id]],dtype=np.int64)\n",
    "        inp_t=tf.convert_to_tensor(inp,tf.int64)\n",
    "        res=mod(inp_t)\n",
    "        probs=tf.squeeze(res).numpy()\n",
    "        predictions.append(probs)\n",
    "    \n",
    "    gold=text_ids[1:]\n",
    "    inp_p=tf.convert_to_tensor(predictions,tf.float32)\n",
    "    cross_entropy = sparse_categorical_crossentropy(gold, inp_p).numpy()\n",
    "    per_word_perplexity = exp(cross_entropy)\n",
    "    avg_perplexity = exp(np.sum(cross_entropy)/cross_entropy.shape[0]).numpy()\n",
    "    \n",
    "    return avg_perplexity, per_word_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity: 74.60694191699187\n",
      "Average perplexity: 112.07748615326427\n"
     ]
    }
   ],
   "source": [
    "news_document='Euroopan parlamentti on hyväksynyt lopullisesti uuden tekijänoikeusdirektiivin .\\\n",
    " Direktiivi meni läpi parlamentin täysistunnossa äänin 348–274 .\\\n",
    " Esitys on ollut poikkeuksellisen kiistanalainen . Sitä ovat puolustaneet oikeuksien haltijat\\\n",
    " kuten musiikintekijät ja lehtikustantajat . Ehdotettua muutosta ovat vastustaneet etenkin internetin\\\n",
    " isot alustapalvelut kuten Google ja Facebook . Esitystä vastaan on käyty myös hyvin laajaa\\\n",
    " kansalaiskampanjointia , jota on motivoinut huoli ilmaisunvapauden rajoituksista .\\\n",
    " Uuden lainsäädännön 13. artiklan mukaan alustapalveluiden on varmistettava , että niiden palveluissa\\\n",
    " ei ole tarjolla materiaalia johon niillä ei ole oikeuksia . Kriitikoiden mukaan esitys johtaa käytännössä\\\n",
    " siihen , että palvelut ryhtyvät varmuuden vuoksi suodattamaan käyttäjiensä niihin lataamaa materiaalia .\\\n",
    " Esitys rajoittaa myös sitä , millaisia otteita tekijänoikeuksien alaisista lehtiartikkeleista saa esittää\\\n",
    " internetissä . Tämä \" linkkiveroksi \" kritisoitu artikla 11 sallii yksittäisten sanojen tai hyvin lyhyiden\\\n",
    " lauseiden linkkaamisen ilman lisensointia . Jäsenmaat ovat jo tahollaan hyväksyneet esityksen , jonka\\\n",
    " käsittely EU:ssa päättyi parlamentin äänestykseen . Suomi vastusti sitä .\\\n",
    " Euroopan parlamentin täysistunto päätti ensin niukalla enemmistöllä , että se ei enää avaa jäsenmaiden\\\n",
    " kanssa jo sovittua ehdotusta muutosesityksille . Sen jälkeen täysistunto hyväksyi direktiivin jäsenmaiden\\\n",
    " kanssa neuvotellussa muodossa . Äänestystä edelsi poikkeuksellisen voimakas kampanjointi esityksen puolesta\\\n",
    " ja sitä vastaan , sillä täysistunnon käsittely nähtiin viimeiseksi tilaisuudeksi vaikuttaa\\\n",
    " tekijänoikeusdirektiivin sisältöön . Esimerkiksi Saksassa 150 000 ihmistä osoitti viikonvaihteessa mieltään\\\n",
    " artikloja 11 ja 13 vastaan eri kaupungeissa .'\n",
    "\n",
    "with open(\"data/fi_tdt-ud-dev.tok.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    tdt_devel=f.read()[:10000]\n",
    "\n",
    "avg_p, word_p = perplexity(news_document, mod, dp)\n",
    "print(\"Average perplexity:\", avg_p)\n",
    "\n",
    "avg_p, word_p = perplexity(tdt_devel, mod, dp)\n",
    "print(\"Average perplexity:\", avg_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnntf",
   "language": "python",
   "name": "rnntf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
